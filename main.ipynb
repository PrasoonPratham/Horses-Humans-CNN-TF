{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details\n",
    "url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip'\n",
    "name = 'data.zip'\n",
    "train_dir = 'data/train/'\n",
    "urllib.request.urlretrieve(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping\n",
    "zipped = zipfile.ZipFile(name, 'r')\n",
    "zipped.extractall(train_dir)\n",
    "zipped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Scale by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Scale by 1/255\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "# Flow from directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 149, 149, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 73, 73, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 35, 35, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "# 1.7 mil parameters ðŸ¤¯\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/g_y_0gnd0v37gfx63299g9_00000gn/T/ipykernel_38681/2970098988.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "33/33 [==============================] - 6s 139ms/step - loss: 0.4605 - accuracy: 0.8218\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 0.2037 - accuracy: 0.9231\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 0.1163 - accuracy: 0.9581\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 0.2583 - accuracy: 0.9698\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 4s 133ms/step - loss: 0.1104 - accuracy: 0.9718\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 0.7680 - accuracy: 0.9776\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 4s 129ms/step - loss: 0.0267 - accuracy: 0.9893\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 9.5717e-04 - accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 9.4963e-05 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 4s 130ms/step - loss: 0.2038 - accuracy: 0.9786\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 0.0186 - accuracy: 0.9922\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 2.2712e-04 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 0.3367 - accuracy: 0.9815\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 8.1471e-04 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 4s 133ms/step - loss: 1.2626e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details\n",
    "url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip'\n",
    "name = 'valid-data.zip'\n",
    "valid = 'data/validation/'\n",
    "urllib.request.urlretrieve(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping\n",
    "zipped = zipfile.ZipFile(name, 'r')\n",
    "zipped.extractall(train_dir)\n",
    "zipped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Validation Image generator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Scale by 1/255\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow from directory\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    valid,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/g_y_0gnd0v37gfx63299g9_00000gn/T/ipykernel_40878/2718311792.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 10:12:48.459400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.5219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 10:13:02.060756: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 17s 486ms/step - loss: 0.7818 - accuracy: 0.5219 - val_loss: 0.6849 - val_accuracy: 0.5117\n",
      "Epoch 2/15\n",
      "33/33 [==============================] - 16s 480ms/step - loss: 0.6353 - accuracy: 0.6349 - val_loss: 1.2688 - val_accuracy: 0.5586\n",
      "Epoch 3/15\n",
      "33/33 [==============================] - 16s 485ms/step - loss: 0.5503 - accuracy: 0.7313 - val_loss: 0.8852 - val_accuracy: 0.6055\n",
      "Epoch 4/15\n",
      "33/33 [==============================] - 16s 476ms/step - loss: 0.4461 - accuracy: 0.7936 - val_loss: 0.5252 - val_accuracy: 0.7578\n",
      "Epoch 5/15\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3664 - accuracy: 0.8403 - val_loss: 1.3074 - val_accuracy: 0.5977\n",
      "Epoch 6/15\n",
      "33/33 [==============================] - 15s 464ms/step - loss: 0.3288 - accuracy: 0.8793 - val_loss: 0.4372 - val_accuracy: 0.8281\n",
      "Epoch 7/15\n",
      "33/33 [==============================] - 16s 481ms/step - loss: 0.1845 - accuracy: 0.9202 - val_loss: 2.1651 - val_accuracy: 0.6250\n",
      "Epoch 8/15\n",
      "33/33 [==============================] - 16s 481ms/step - loss: 0.2780 - accuracy: 0.9114 - val_loss: 0.9628 - val_accuracy: 0.7422\n",
      "Epoch 9/15\n",
      "33/33 [==============================] - 16s 486ms/step - loss: 0.2244 - accuracy: 0.9309 - val_loss: 1.8371 - val_accuracy: 0.5742\n",
      "Epoch 10/15\n",
      "33/33 [==============================] - 16s 489ms/step - loss: 0.2019 - accuracy: 0.9279 - val_loss: 0.2575 - val_accuracy: 0.9180\n",
      "Epoch 11/15\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.1832 - accuracy: 0.9328 - val_loss: 1.2754 - val_accuracy: 0.7070\n",
      "Epoch 12/15\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.1197 - accuracy: 0.9542 - val_loss: 2.6182 - val_accuracy: 0.6250\n",
      "Epoch 13/15\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.2503 - accuracy: 0.9435 - val_loss: 1.5525 - val_accuracy: 0.6758\n",
      "Epoch 14/15\n",
      "33/33 [==============================] - 16s 478ms/step - loss: 0.1104 - accuracy: 0.9611 - val_loss: 2.9099 - val_accuracy: 0.6484\n",
      "Epoch 15/15\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.1514 - accuracy: 0.9494 - val_loss: 3.1256 - val_accuracy: 0.6328\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=valid_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def predict_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    image_tensor = np.vstack([x])\n",
    "    classes = model.predict(image_tensor)\n",
    "    print(classes)\n",
    "    print(classes[0])\n",
    "\n",
    "    if classes[0] > 0.5:\n",
    "        print('Prediction: Human')\n",
    "    else:\n",
    "        print('Prediction: Horse')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "326012475e3b207bfe43a178e06d3534477db45228184ee49f03c9b3d2f59f0f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
